{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMIlIZ1o7ihKjje2433uV3w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 8. Topic Modeling\n","\n","Topic modeling is a technique in natural language processing (NLP) that helps uncover the underlying themes or topics in a collection of documents. This can be incredibly useful for summarizing, organizing, and understanding large volumes of text.\n"],"metadata":{"id":"lqzUIrZjQ0jE"}},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"McDx5bt5Qrwc","executionInfo":{"status":"ok","timestamp":1723729721374,"user_tz":-60,"elapsed":6817,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"7dbd53a5-79f9-4492-99fc-d2d97a0c1f7b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Collecting pyLDAvis\n","  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n","Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.1.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.1)\n","Collecting funcy (from pyLDAvis)\n","  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (71.0.4)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.5)\n","Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n","Installing collected packages: funcy, pyLDAvis\n","Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"]}],"source":["# Prepare Your Environment\n","\n","!pip install nltk scikit-learn gensim matplotlib pyLDAvis\n"]},{"cell_type":"code","source":["#  Load and Preprocess the Data\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import RegexpTokenizer\n","import string\n","\n","# Download necessary NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Sample documents\n","documents = [\n","    \"Machine learning is fascinating.\",\n","    \"Natural language processing enables computers to understand human language.\",\n","    \"Topic modeling is a technique used to discover abstract topics.\",\n","    # Add more documents\n","]\n","\n","# Tokenization and preprocessing function\n","def preprocess_text(text):\n","    # Tokenization\n","    tokenizer = RegexpTokenizer(r'\\w+')\n","    tokens = tokenizer.tokenize(text.lower())\n","\n","    # Remove stop words\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    # Stemming\n","    stemmer = PorterStemmer()\n","    tokens = [stemmer.stem(word) for word in tokens]\n","\n","    return tokens\n","\n","# Preprocess all documents\n","preprocessed_docs = [preprocess_text(doc) for doc in documents]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pes57_q_TKlB","executionInfo":{"status":"ok","timestamp":1723729756895,"user_tz":-60,"elapsed":253,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"040b61bf-ee7d-4044-9545-746d229640a8"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["#  Create a Document-Term Matrix\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Convert preprocessed docs back to text for CountVectorizer\n","processed_docs_text = [' '.join(doc) for doc in preprocessed_docs]\n","\n","# Create Document-Term Matrix\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(processed_docs_text)\n","\n","# Get feature names\n","feature_names = vectorizer.get_feature_names_out()\n"],"metadata":{"id":"w5Fk61tRTMCc","executionInfo":{"status":"ok","timestamp":1723729784036,"user_tz":-60,"elapsed":291,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Apply Topic Modeling\n","\n","from sklearn.decomposition import LatentDirichletAllocation\n","\n","# Number of topics\n","num_topics = 3\n","\n","# Apply LDA\n","lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n","lda.fit(X)\n","\n","# Get topic-word distribution\n","topic_word_dist = lda.components_\n"],"metadata":{"id":"gIhhulnPTSpX","executionInfo":{"status":"ok","timestamp":1723729797439,"user_tz":-60,"elapsed":303,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# . Examine the Topics\n","\n","# Display top words for each topic\n","num_words = 10\n","for topic_idx, topic in enumerate(topic_word_dist):\n","    top_words_idx = topic.argsort()[-num_words:][::-1]\n","    top_words = [feature_names[i] for i in top_words_idx]\n","    print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TiHdSNifTV64","executionInfo":{"status":"ok","timestamp":1723729920740,"user_tz":-60,"elapsed":359,"user":{"displayName":"Babu Pallam","userId":"10131957121692699069"}},"outputId":"9920c0ac-eb28-41b0-f5c8-f5f9b0f6b47e"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Topic 0: languag, understand, process, natur, human, enabl, comput, fascin, machin, learn\n","Topic 1: topic, use, techniqu, abstract, model, discov, machin, learn, fascin, understand\n","Topic 2: machin, learn, fascin, understand, process, natur, human, enabl, comput, languag\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"fOMbnRywTc4f"},"execution_count":null,"outputs":[]}]}